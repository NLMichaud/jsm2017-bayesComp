
\section{MCMC}


\frame{
\sffamily
\frametitle{Markov chain Monte Carlo Methods}
\begin{itemize}
\item Motivation
\item Brief review of Markov chain theory
\item Metropolis Hastings algorithms.
\item Diagnostics.
\item Gibbs sampling.
\item General design rules.
\item Adaptive MCMC.
\item Slice samplers.
\item Hamiltonian MCMC.
\item Reversible Jump MCMC.
\end{itemize}
}




\frame{
\sffamily
\frametitle{Motivation for MCMC algorithms}
\begin{itemize}
\item Generally speaking, the direct sampling algorithms discussed before do not scale well as the number of parameters grow.

\item In \textit{optimization} problems, this ``curse of dimensionality'' is dealt with by using conjugate gradient algorithms.
\end{itemize}
\vspace{-8mm}
\begin{center}
\begin{tabular}{lr}
\begin{minipage}{5.2cm}
Conjugate gradient is an iterative algorithm that repeatedly optimizes along one single dimension while keeping the value of the rest fixed.
\end{minipage}
&
\begin{minipage}{5.2cm}
\begin{center}
\includegraphics[height=5.4cm,angle=0]{plots/rconjugategradient.pdf}\\
\end{center}
\end{minipage}
\end{tabular}
\end{center}
}






\frame{
\sffamily
\frametitle{Motivation for MCMC algorithms}
\begin{itemize}
\item {\bf Example (Gaussian distributions with unknown mean and variance):}  Assume that data $y_1, \ldots, y_n$ are independent and identically distributed with $y_i \mid \theta, \sigma^2 \sim \normal(\theta, \sigma^2)$ and that we use independent priors $\theta \sim \normal (\mu, \tau^2)$ and $\sigma^2 \sim \IGam(a, c)$.  After some algebra the posterior distribution reduces to
\begin{multline*}
p(\theta, \sigma^2 \mid y_1, \ldots, y_n) \propto \left(\frac{1}{\sigma^2}\right)^{\frac{n}{2} + a + 1} \\
%
\exp\left\{ -\frac{n\theta^2 - 2n\theta \bar{y} + \sum_{i=1}^{n} y_i^2 + 2c}{2\sigma^2} - \frac{\theta^2 - 2\theta\mu + \mu^2}{2\tau^2}   \right\} .
\end{multline*}
This posterior is intractable (by which I mean, computing means/variances and cumulative probabilities of events is difficult).

\end{itemize}
}





\frame{
\sffamily
\frametitle{Motivation for MCMC algorithms}
\begin{itemize}
\item {\bf Example (Gaussian distributions with unknown mean and variance, cont):}  However, we can try to find the posterior mode (which would provide us with a point estimate for $(\theta, \sigma^2)$ in the same spirit as the MLE) using iterative conditional modes.  

\vspace{1mm}

Starting with some initial guess $\left(\theta^{(0)}, \sigma^{2(0)} \right)$ (such as $\left(\theta^{(0)}, \sigma^{2(0)} \right) = \left( \bar{y}, \Var(y) \right)$), iterate through:
\begin{enumerate}
\item Set $\theta^{(b)} = \frac{\frac{n \bar{y}}{\sigma^{2(b-1)}} + \frac{\mu}{\tau^2}}{\frac{n}{\sigma^{2(b-1)}} + \frac{1}{\tau^2}}$.
\item Set $\sigma^{2(b)} = \frac{2 b + \sum_{i=1}^{n}\left(y_i - \theta^{(b)} \right)^2}{n + 2a + 2}$.
\end{enumerate}
The algorithm is stopped when the value of the posterior ``does not change'' from one iteration to the next.

\vspace{1mm}

See the file \texttt{simpleconjugategradient.R}.
\end{itemize}
}






\frame{
\sffamily
\frametitle{Motivation for MCMC algorithms}
\begin{itemize}
\item Conjugate gradient only gives us a point estimator and does not allow for full exploration of the posterior distribution (e.g., to compute credible intervals).  

\item What would be an equivalent idea for \textit{sampling} from a distribution $p(\bftheta)$ with $\bftheta = (\theta_1, \ldots, \theta_p)$?
\begin{itemize}
\item If we consider a single dimension at a time we end up working with the \textit{full conditional} $p(\theta_i \mid \theta_1, \ldots, \theta_{i-1}, \theta_{i+1}, \ldots, \theta_{p})$.
\item Because we are trying to sample, maybe we can get a new value of $\theta_i$ by simulating from $p(\theta_i \mid \theta_1, \ldots, \theta_{i-1}, \theta_{i+1}, \ldots, \theta_{p})$ instead of by maximizing it.
\end{itemize}

\item Does this work?
\begin{itemize}
\item Generally speaking, the answer is yes, this is the Gibbs sampler!
\end{itemize}

\end{itemize}
}



\frame{
\sffamily
\frametitle{Motivation for MCMC algorithms}
\begin{itemize}
\item {\bf Example (Gaussian distributions with unknown mean and variance, cont):}  We do not know how to sample directly from the posterior distribution.  However:
\begin{align*}
\theta \mid \sigma^2, y_1, \ldots, y_n \sim \normal \left( 
\frac{\frac{n \bar{y}}{\sigma^2} + \frac{\mu}{\tau^2}}{\frac{n}{\sigma^2} + \frac{1}{\tau^2}}  ,
%
\frac{1}{\frac{n}{\sigma^2} + \frac{1}{\tau^2}}
\right) ,
\end{align*}
and
\begin{align*}
\sigma^2 \mid \theta, y_1, \ldots, y_n \sim \IGam \left( 
a + \frac{n}{2} ,
%
c + \frac{1}{2} \sum_{i=1}^{n} (y_i -\theta)^2 
\right) .
\end{align*}
\end{itemize}
(To find these full conditionals, just drop from the posterior all the -- multiplicative --  terms that do not involve the parameter of interest, and then try to recognize what is left as the kernel of a known distribution.) 
}


\frame{
\sffamily
\frametitle{Motivation for MCMC algorithms}
\begin{itemize}
\item {\bf Example (Gaussian distributions with unknown mean and variance, cont):}  The iterative algorithm looks like this

\begin{enumerate}
\item Initialize $\tilde{\theta}^{(0)}$ and $\tilde{\sigma}^{2(0)}$.
\item For $b=1,\ldots, B$ repeat
\begin{enumerate}
\item Sample $\tilde{\theta}^{(b)} \sim \normal \left( 
\frac{\frac{n \bar{y}}{\tilde{\sigma}^{2(b-1)}} + \frac{\mu}{\tau^2}}{\frac{n}{\tilde{\sigma}^{2(b-1)}} + \frac{1}{\tau^2}}  ,
%
\frac{1}{\frac{n}{\tilde{\sigma}^{2(b-1)}} + \frac{1}{\tau^2}}
\right)$.
\item Sample $\tilde{\sigma}^{2(b)} \sim  \IGam \left( 
a + \frac{n}{2} ,
%
c + \frac{1}{2} \sum_{i=1}^{n} \{y_i -\tilde{\theta}^{(b)}\}^2 
\right)$.
\end{enumerate}
\end{enumerate}

\vspace{1mm}

An example of this implementation in R is available in the file \texttt{simpleGibbs.R}.
\begin{itemize}
\item Run the algorithm first with $\theta^{(0)} = 98$ and $\sigma^{2(0)} = 1$ and then with $\theta^{(0)} = 0$ and $\sigma^{2(0)} = 1600$.  What differences do you see in the trace plots?
\end{itemize}
\end{itemize}
}



\frame{
\sffamily
\frametitle{Motivation for MCMC algorithms}
\begin{itemize}
\item A couple of things to keep in mind:

\begin{itemize}
\item Unlike conjugate gradient, we care about all numbers generated by the sampler, not only about the last one (we are trying to explore the whole distribution, not just find where the mean/median/mode is).

\vspace{2mm}

\item Note that when the initial values are too far from where most of the mass of the posterior is located (as in the second set of initial values), the first few samples are clearly wrong and can seriously bias the results. Hence they need to be discarded.
\end{itemize}
\end{itemize}
}



\frame{
\sffamily
\frametitle{Motivation for MCMC algorithms}
\begin{itemize}
\item Because in the Gibbs sampler we are dealing with a distribution, we need to think about converge in the probabilistic sense (e.g., convergence in probability or convergence in distribution) rather than in terms of  convergence in the sense we talk in analysis.

\vspace{2mm}

\item Because the value of each pair $\left(\theta^{(b)}, \sigma^{2(b)}\right)$ depends on the value of $\left(\theta^{(b-1)}, \sigma^{2(b-1)}\right)$ (but NOT on $\left(\theta^{(b-2)}, \sigma^{2(b-2)}\right) , \ldots , \left(\theta^{(1)}, \sigma^{2(1)}\right)$) we are actually dealing with a Markov chain!
\end{itemize}
}



\frame{
\sffamily
\frametitle{Motivation for MCMC algorithms}
\begin{itemize}
\item The Gibbs sampler is but one example of a class of algorithms that attempt to construct Markov chains whose limiting distributions correspond to the posterior distribution of interest, and then iterate the 
chain to generate (dependent) samples that can be used to approximate any integral of interest.  Hence the name of Markov chain Monte Carlo methods!

\vspace{2mm}

\item For a history of MCMC algorithms, see \cite{robert2011short}.
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item A \textit{Markov chain} is a sequence of random variables $X_1, X_2, X_3 \ldots$ defined on a common probability space $(\Omega, \mathcal{B})$ such that the distribution of $X_t$ given $X_{t-1}, \ldots, X_{1}$ depends only on $X_{t-1}$, i.e., 
\begin{multline*}
\Pr(X_t \in A \mid X_{t-1} = x_{t-1}, \ldots, X_1 = x_1) = \\
%
\Pr(X_t \in A \mid X_{t-1} = x_{t-1})
$$
\end{multline*}

\item It is convenient to think of $t$ as representing time.
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item Markov chains are defined in terms of \textit{transition kernels}.

\vspace{2mm}

\item A \textit{transition kernel} is a function $K$ defined on $\Omega \times \mathcal{B}$ such that
\begin{enumerate}
\item For all $x \in \Omega$, $K(x,\cdot)$ is a probability measure.
\item For all $A \in \mathcal{B}$, $K(\cdot, A)$ is measurable.
\end{enumerate}

\vspace{2mm}

\item We will focus on \textit{time-homogeneous chains} where the transition kernel is the same for all $t$.
\begin{itemize}
\item When $\Omega$ is countable the transition kernel is a matrix such that $[\bfP ]_{x,y} = \Pr(X_t= y \mid X_{t-1} = x)$ where $x,y \in \Omega$.

\vspace{2mm}

\item When $\Omega$ is uncountable $K$ represents a conditional density, i.e., $\Pr(X_t \in A | X_{t-1} = x) = \int_A K(x,x') \dd x'$.
\end{itemize}
\end{itemize}
}




\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item For now we will focus on countable spaces, so we will be working with transition matrices.  The results and insights for continuous spaces are similar, but the theory is (logically) more involved.

\vspace{1mm}

\item Besides providing some intuition without a lot of mathematical complexity, problems with discrete sampling spaces can be of interest in their own right!
\begin{itemize}
\item You might think that countable spaces are just a toy example and are not really relevant for practical applications.  However, you would be wrong!
\end{itemize}
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}

Back to our review of (time homogeneous, discrete state-space) Markov chains ...

\begin{itemize}
\item Assuming that $\Omega$ contains exactly $K$ points, the state of a chain at time $t$ can be represented by a probability vector $\bfpi_t = (\pi_{t,1}, \ldots, \pi_{t,K})^T$.

\vspace{2mm}

\item The state of the chain at times $t$ and $t+1$ are related by $\bfpi_{t}^{T} \bfP = \bfpi_{t+1}^T$.

\vspace{2mm}

\item Similarly, the state of the chain at times $t$ and $t+k$ are related by $\bfpi_{t}^{T} \bfP^{k} = \bfpi_{t+1}^T$.
\end{itemize}
}


\frame{
\sffamily
\frametitle{Stationary distribution of a Markov chain}
\begin{itemize}
\item A distribution $\tilde{\bfpi}$ is a \textit{stationary distribution} if $\tilde{\bfpi}^{T} \bfP = \tilde{\bfpi}^T$.

\item A Markov chain (on a countable probability space) has a unique stationary distribution if and only if it is \textit{irreducible} and all of its states are \textit{positive recurrent}.
\begin{itemize}
\item A chain is irreducible if for every pair of states $i$, $j$  exist finite constants $n_{i,j}$ and $n_{j,i}$ such that $\Pr(X_{n_{i,j}} = j \mid X_0 = i) > 0$ and $\Pr(X_{n_{j,i}} = i \mid X_0 = j) > 0$.

\item A state is positive recurrent if the time it takes to return to it once you leave it is finite, i.e., if
\begin{align*}
M_i &= \sum_{t=1}^{\infty} t \, f_{ii}^{t} < \infty
\end{align*}
where
\begin{align*}
f_{ii}^{t} &= \Pr(T_i = t) ,   &   T_i = \inf \left\{ t \ge 1 : X_t=i \mid X_0 = i \right\} .
\end{align*}

\end{itemize}
\end{itemize}
}




\frame{
\sffamily
\frametitle{Limiting distribution of a Markov chain}
\begin{itemize}
\item A distribution $\hat{\bfpi}$ is called a \textit{limiting distribution} (or \textit{equilibrium distribution}) if for any $\bfpi_0$ we have $\lim_{n \to \infty} \bfpi_0^{T} \bfP^n = \hat{\bfpi}$.

\item A Markov chain has a limiting distribution if the chain is positive recurrent chain, irreducible and aperiodic.
\begin{itemize}
\item A state $i$ has period $k$ if any return to state $i$ must occur in multiples of $k$ time steps, i.e., $k=\mbox{gcd} \left\{ n : \Pr(X_n=i \mid X_0=i) > 0 \right\}$
\item A state with period $k=1$ is called aperiodic.
\item An aperiodic chain has all its states being aperiodic.
\end{itemize}

\item If a chain has a limiting distribution then it is equal to the unique stationary distribution!
\end{itemize}
}




\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item {\bf Example:}  The chain with transition matrix 
$$
\left( \begin{matrix}
0 & 1 \\
1 & 0 
\end{matrix}
\right)
$$
is irreducible and all of its states are positive recurrent, but it is periodic (with period 2).  Accordingly, note that $\tilde{\bfpi} = (1/2, 1/2)^T$ is the unique stationary distribution for this chain, but it does not have an equilibrium distribution because
$$
(\pi_0 , 1 - \pi_0) 
\left( \begin{matrix}
0 & 1 \\
1 & 0 
\end{matrix}
\right)^{n}
= \begin{cases}
(\pi_0 , 1 - \pi_0) & n \mbox{ even} \\
(1 - \pi_0 , \pi_0) & n \mbox{ odd} 
\end{cases} ,
$$
so the limit as $n \to \infty$ does not exist.
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item The previous discussion should hint at what we want to do.  If we can construct a Markov chain that has a unique stationary/limiting distribution that is identical to the one we are interested in (in our case, the posterior distribution), then we can generate samples from the posterior by using the following approach:
\begin{itemize}
\item Pick an initial state from the chain $x_0$ according to some distribution (often the prior).

\item For $t = 1, \ldots, T$ iteratively sample $x_t$ from a multinomial distribution with parameter $(p_{x_{t-1}, 1}, \ldots, p_{x_{t-1}, K})$.

\item Use the samples so obtained to approximate any posterior summary of interest.
\end{itemize}
\end{itemize}
}




\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item Some caveats!
\begin{itemize}
\item The samples generated by the chain will be (approximately) identically distributed.  \alert{However, they will not be independent}, so convergence of empirical averages to integrals cannot be argued using standard arguments (e.g., WLLN).  Instead, the \textit{ergodic theorem} needs to be used to justify the convergence of averages.

\item Additionally, because samples are not independent, the variance of the estimates will depend not only on how many samples are generated, but also on the autocorrelation!  This means we might need more samples than if we had a direct sampler.
 
\item Unless the initial state is sampled from the stationary distribution (which will rarely be the case) the first few samples will follow a distribution that can be very different from the stationary distribution (and therefore need to be discarded!)
%\begin{itemize}
%\item How many samples need to be discarded (we call this the ``burn-in'' period) depends both on how far $\pi_0$ is from $\hat{\pi}$ and how large the second eigenvalue of $\bfP$ is.
%\end{itemize}
\end{itemize}

\end{itemize}
}




\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item How fast do we converge to $\hat{\bfpi}$? Assume that a Markov chain is irreducible, aperiodic and positive recurrent.  Also, assume that $\bfP$ is diagonalizable (a slightly more convoluted argument can be made when $\bfP$ is not diagonalizable, but I want to keep it simple here).

\item In this case we can study the rate of convergence toward that limiting distribution by using the spectral decomposition of $\bfP$,
$$
\bfP = \bfD \bfLambda \bfD^{-1} = \sum_{k=1}^{K} \lambda_k \bfu_i \bfv_i^{T}
$$
where $\bfLambda$ is a diagonal matrix containing the eigenvalues of $\bfP$, $\bfD$ correspond to the eigenvectors of $\bfP$, $\bfu_i^T$ is the $i$-th row of $\bfD$ and $\bfv_i^T$ is the $i$-th row of $\bfD^{-1}$.

\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item Without loss of generality, we assume that the eigenvalues are ordered so that $|\lambda_1| > |\lambda_2| > \ldots > |\lambda_K|$.

\item Because $\bfP$ is row-stochastic its largest eigenvalue is 1, i.e., $\lambda_1 = 1$.  Also, because we know that the stationary distribution is unique, the multiplicity of this eigenvalue has to be one.  Hence the remaining eigenvalues satisfy $|\lambda_k| < 1$ for all $k \ge 2$ and therefore $\lim_{n \to \infty} \lambda_k^n \to 0$ for $k \ge 2$
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item Now, note that 
$$
\bfP^n = \bfD \bfLambda^{n} \bfD^{-1}= \sum_{k=1}^{K} \lambda_k^{n} \bfu_i \bfv_i^{T} =  \bfu_1 \bfv_1^{T} + \sum_{k=2}^{K} \lambda_k^{n} \bfu_i \bfv_i^{T}
$$

\item Hence $\lim_{n \to \infty} \bfP^{n} = \bfu_1 \bfv_1^{T}$ and the rows of $\bfu_1 \bfv_1^{T}$ are all identical and correspond to the stationary/limiting distribution.

\item Also, the rate of convergence is dominated by $|\lambda_2|$, the second largest eigenvalue (in absolute value).
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{center}
\alert{We care about the rate of convergence because there are many chains with the same stationary/limiting distribution, and the rate of convergence potentially allows us to select among them.}
\end{center}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item {\bf Example:  }  Consider constructing the family of two-states Markov chains with transition distribution
$$
\bfP = \left(\begin{matrix}
1-\alpha  &  \alpha \\
\beta  &  1 - \beta
\end{matrix}\right)
$$  
It is easy to verify that the stationary distribution of this chain is $\left( \frac{\beta}{\alpha + \beta} , \frac{\alpha}{\alpha + \beta} \right)$.  Hence, all two-state chains with $\alpha = \beta$ have stationary distribution $\left( \frac{1}{2}, \frac{1}{2} \right)$.  How fast do they converge to the limiting distribution?
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item {\bf Example (cont):  }  The spectral decomposition of the transition matrix in this case is
$$
\bfP = \left(\begin{matrix}
\frac{1}{2}  &  \frac{1}{2} \\
\frac{1}{2}  &  \frac{1}{2}
\end{matrix}\right) + 
%
(1 - 2\alpha)
%
\left(\begin{matrix}
\frac{1}{2}  &  -\frac{1}{2} \\
-\frac{1}{2}  &  \frac{1}{2}
\end{matrix}\right)
$$
so that 
$$
\bfP^{n} = \left(\begin{matrix}
\frac{1}{2}  &  \frac{1}{2} \\
\frac{1}{2}  &  \frac{1}{2}
\end{matrix}\right) + 
%
(1 - 2\alpha)^{n}
%
\left(\begin{matrix}
\frac{1}{2}  &  -\frac{1}{2} \\
-\frac{1}{2}  &  \frac{1}{2}
\end{matrix}\right)
$$
when $\alpha = 1/2$ we have $\bfP^n = \bfP$ as expected (when $\alpha = 1/2$ we have an independent sampler).
\end{itemize}
}




\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item {\bf Example (cont):  }  More generally, if we assume that $\pi_{t-1} = \tilde{\pi}$ then
$$
\Cor(X_t, X_{t-1}) = (1- 2\alpha)
$$
(the second eigenvalue!!).  Hence, when $\alpha \approx 0$ we have a chain with high positive autocorrelation that converges very slowly to the stationary distribution, while $\alpha \approx 1$ leads to a chain with strong negative autocorrelation that tends to very often alternate among states.
\end{itemize}
}






\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item {\bf Example (cont):  }  These are realizations of the Markov chain for different values of $\alpha$
\end{itemize}
\includegraphics[height=3.75cm,angle=0]{plots/markovchain1.pdf}
\includegraphics[height=3.75cm,angle=0]{plots/markovchain5.pdf}
\includegraphics[height=3.75cm,angle=0]{plots/markovchain9.pdf}
\begin{itemize}
\item In all cases about half of the point are at state 1 and half at state 2, but the dependence changes dramatically!
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item {\bf Example (cont):  }  Which one is better for estimating $\Pr(X_t=1)$?  At first sight it would seem that $\alpha = 0.5$ (no autocorrelation) is the best.  However, you might argue that $\alpha =0.9$ (negative autocorrelation) is better.   To see why, assume that the initial state is drawn from the stationary distribution and consider the empirical estimate of the probability of state 1, $\frac{1}{T} \sum_{b=1}^{T} \ind_{(X_t = 1)}$.
\begin{multline*}
\Var \left\{ \frac{1}{T} \sum_{b=1}^{T} \ind_{(X_t = 1)} \right\} = \\
%
\frac{\Var(X_0)}{T} + \frac{2\Var(X_0)}{T^2} \sum_{i=1}^{T} \sum_{j=i+1}^{T} \Cor(X_i, X_j)
\end{multline*}
$\sum_{i=1}^{T} \sum_{j=i+1}^{T} \Cor(X_i, X_j)$ is decreasing with $\alpha$.
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item \alert{Negative correlations are actually good for Monte Carlo integration (because, although we still need to worry about convergence, they reduce the variance of the estimators).  }

\item However, most of the algorithms we employ in Bayesian statistics produce chains that are positively autocorrelated, so we will typically frown upon autocorrelation.
\end{itemize}
}


\frame{
\sffamily
\frametitle{A brief review of Markov chains}
{\bf Suggested exercise:  } Compute $\Cor(X_t, X_{t+k})$ for $k \ge 1$ if $X_t \sim \tilde{\pi}$. (Recall that $\Cor(X_t, X_{t+1}) = (1-2\alpha)$.)
}


\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item Given a transition matrix, finding the stationary/limiting distribution is not too difficult.  However, in Bayesian statistics we are faced with the opposite problem:  we know the stationary/limiting distribution and need to come up with (at least one) transition kernel that generates it.

\item Reversibility is a concept that can help in the construction of such chains.
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
\begin{itemize}
\item A Markov chain with transition matrix $\bfP$ and stationary distribution $\tilde{\bfpi}$ is said to be \textit{reversible} if $\tilde{\pi}_i p_{i,j} = \tilde{\pi}_j p_{j,i}$.  This equation is called the \textit{detailed balance equation}.

\item Roughly speaking, If we know what the stationary distribution should look like, the detailed balance equations tell us that we can choose $p_{i,j}$ for $i > j$ in whichever way we want as long we then make $p_{j,i} = \frac{\pi_{i}}{\pi_{j}} p_{i,j}$.  (Of course, recall that the transition matrix still has to be row stochastic ...).
\end{itemize}
}



\frame{
\sffamily
\frametitle{A brief review of Markov chains}
{\bf Suggested exercise:  }  Construct one reversible and one non-reversible three-state chain with limiting distribution $\left( \frac{1}{3}, \frac{1}{3}, \frac{1}{3} \right)$.
}



