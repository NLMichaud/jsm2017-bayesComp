

\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item  As our Metropolis examples illustrate, one of the main challenges associated with designing MH  algorithms is selecting the proposal distribution.

\item In particular, concentrate on (Gaussian) random-walk MH algorithms, where the problem is how to select the variance-covariance matrix.  What we have done so far requires a lot of back and forth!

\item Is there a way in which we can create a ``self-tuning'' algorithm?  This is the main thrust behind \alert{adaptive MCMC algorithms}!!!
\end{itemize}
}



\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item A simple yet powerful idea (at least for elliptical posteriors):  start with a potentially very bad covariance matrix and, after the algorithm has been running for a while, switch to different proposal that uses the previous values of the chain to estimate $\Cov( \bftheta \mid \bfy)$ (just as we did when we manually tuned the chain).

\item This is particularly useful when $\dim\{ \bftheta \} = d$ is large!

\item  The main conceptual issue with this approach is that using the whole history of the chain to create a proposal means that we are not working with a Markov chain anymore (or, at least, not with a time-homogenous one), so the theory we discussed so far cannot be used to ensure that the algorithm actually converges to the desired equilibrium distribution.
\end{itemize}
}




\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item {\bf Example (taken from \citealp{RoRo:09} and \citealp{haario2001adaptive}):  }  Consider a target distribution $\normal(\mathbf{0}, \bfM \bfM^T)$ where the entries of $\bfM$ have been randomly generated from standard normal distribution.

\vspace{1mm}

We consider the case $d=\dim\{ \bftheta \} = 10$.  For the first $20,000$ iterations, the proposal distribution is fixed and corresponds to a normal distribution, $q(\vartheta \mid \theta) = \normal\left( \vartheta \mid \theta, \frac{0.1^2}{d} \mathbf{I} \right)$.  After that point, the proposal becomes a mixture
$$
q(\vartheta \mid \theta) = (1- \beta) \normal \left(\vartheta \mid \theta, \frac{2.38^2}{d} \bfSigma_b \right) + \beta \normal\left(\vartheta \mid \theta, \frac{0.1^2}{d} \mathbf{I} \right)
$$
where $\bfSigma_b$ is the current empirical estimate of the covariance of the target distribution (computed on the basis of the previous $b-1$ iterations of the chain), and $\beta > 0$ is a constant.
\end{itemize}
}





\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item {\bf Example (taken from \citealp{RoRo:09} and \citealp{haario2001adaptive}, cont):  }  The algorithm is implemented in the file \texttt{adaptive1.R}, which allows you to run the algorithm using only the ``default'', only the ``optimal'', and the adaptive proposal.  The graphs below show the trace plots for the first component of $\bftheta$. 

\includegraphics[height=3.3cm,angle=0]{plots/adap_trace_def.pdf}
\includegraphics[height=3.3cm,angle=0]{plots/adap_trace_opt.pdf}
\includegraphics[height=3.3cm,angle=0]{plots/adap_trace_adp.pdf}


\end{itemize}
}




\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item {\bf Example (taken from \citealp{RoRo:09} and \citealp{haario2001adaptive}, cont):  }    The graphs below show the acf functions for the first component of $\bftheta$.

\includegraphics[height=3.3cm,angle=0]{plots/adap_acf_def.pdf}
\includegraphics[height=3.3cm,angle=0]{plots/adap_acf_opt.pdf}
\includegraphics[height=3.3cm,angle=0]{plots/adap_acf_adp.pdf} \\


Acceptance rates are 0.964, 0.264, and 0.260.  The effective sample sizes in the last two cases are 3096 and 3200.
\end{itemize}
}



\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item {\bf Example (taken from \citealp{RoRo:09} and \citealp{haario2001adaptive}, cont):  }  

\fix Abel, I revised language here to reflect more dynamic adaptation schemes currently used, including Nimble's scheme; see what you think

Some things to consider:
\begin{itemize}
\item The use of $20,000$ ``preliminary'' samples and a ``default'' variance $ \frac{0.1^2}{d} \mathbf{I}$ are totally ad-hoc.

\item More recent schemes (including that in NIMBLE) adapt on the fly (e.g., every 100 iterations), averaging the current proposal covariance with the estimated posterior covariance from recent iterations.

\item Adaptive univariate schemes simply adjust the proposal scale.

\end{itemize}
\end{itemize}
}

\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}

NIMBLE's adaptive MCMC
\begin{itemize}
\item NIMBLE's default scalar and block Metropolis samplers use adaptation. 
\item Users can set the initial proposal scale / covariance.
\item Current scheme can sometimes perform very poorly in some cases where scales of initial proposal covariance and posterior covariance are very different.
\begin{itemize}
\item We plan to roll out a fix for this in the next release.
\end{itemize}
\end{itemize}
}

\begin{frame}[fragile]
\sffamily
\frametitle{Adaptive MCMC algorithms}

\begin{itemize}
\item {\bf Example (Nonlinear binomial regression):} in \texttt{mh-bliss.R} we have NIMBLE code for non-adaptive (theoretical proposal covariance) and adaptive blocked and univariate Metropolis sampling. 

\includegraphics[width=3.5in]{plots/adaptive-bliss}
\end{itemize}

\end{frame}

\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item Posterior distributions that are approximately elliptical are not the most thrilling application of adaptive MCMC algorithms because something very similar can be accomplished ``manually'' without lots of extra effort.

\item More interesting applications involve ``banana-shaped'' posteriors in which you would potentially need a different proposal distribution depending on where in the space you are!
\end{itemize}

\fix Consider whether we want to move the banana example to the HMC section; previously it nicely transitioned to HMC

}




\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{center}
\includegraphics[height=5.2cm,angle=0]{plots/banana_density.pdf}
\includegraphics[height=5.2cm,angle=0]{plots/banana_density2.pdf}
\end{center}
}





\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item {\bf Example (the bivariate ``banana'' distribution, taken from \citealp{andrieu2008tutorial}):  }  Consider the distribution of $(\theta_1, \theta_2) = (x_1, x_2+b\{x_1^2 -100\})$ where $(x_1, x_2) \sim \normal\left(0 , \diag\{ 100, 1 \} \right)$.  The density is proportional to
$$
\exp\left\{- \frac{\theta_1^2}{200} - \frac{\left(\theta_2 + b\left[\theta_1^2 - 100 \right]\right)^2}{2} \right\}
$$

We first run a regular bivariate Gaussian random walk MCMC with proposal variance $\diag\{ 2, 2 \}$.

\end{itemize}

}




\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item {\bf Example (the bivariate ``banana'' distribution, taken from \citealp{andrieu2008tutorial}), cont:  }  The acf for each of the two parameters are shown here:
\end{itemize}
\includegraphics[height=5.2cm,angle=0]{plots/banana_default_acf1.pdf}
\includegraphics[height=5.2cm,angle=0]{plots/banana_default_acf2.pdf}
}



\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item {\bf Example (the bivariate ``banana'' distribution, taken from \citealp{andrieu2008tutorial}), cont:  }  The trace plots for each of the two parameters are shown here:
\end{itemize}
\includegraphics[height=5.2cm,angle=0]{plots/banana_default_trace1.pdf}
\includegraphics[height=5.2cm,angle=0]{plots/banana_default_trace2.pdf}
}



\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item {\bf Example (the bivariate ``banana'' distribution, taken from \citealp{andrieu2008tutorial}), cont:  }  This is the joint distribution:
\end{itemize}
\begin{center}
\includegraphics[height=6.0cm,angle=0]{plots/banana_sample_naive.pdf}
\end{center}
}



\frame{
\sffamily
\frametitle{Adaptive MCMC algorithms}
\begin{itemize}
\item {\bf Example (the bivariate ``banana'' distribution, taken from \citealp{andrieu2008tutorial}), cont:  }  It should be clear that a single Gaussian proposal cannot work well in this case, no matter how much we tune its variance (adaptively or not).

\vspace{1mm}

You need a proposal with a positive correlation for negative values of $\theta_1$ and with negative correlation for positive values. However, the ``optimal'' covariance matrix (in the sense we have discussed so far) has zero correlation!

\vspace{1mm}

One possible solution:  Use a mixture distribution as your proposal, with weights for the mixture that depend on the current state of the chain.  The covariance of the proposal for each component can be adaptively updated similarly to the one-component case.
\end{itemize}
}



